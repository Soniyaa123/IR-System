{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "wSxABRrR-10m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk . download ('stopwords')\n",
        "nltk . download ('punkt')\n",
        "nltk . download ('wordnet')\n",
        "import os\n",
        "import string\n",
        "import logging\n",
        "import re\n",
        "from collections import defaultdict , Counter\n",
        "from nltk . corpus import stopwords\n",
        "from nltk . tokenize import word_tokenize\n",
        "from nltk . stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "STOPWORDS = set( stopwords . words ('english') )\n",
        "STOPWORDS.remove('and')\n",
        "STOPWORDS.remove('or')\n",
        "STOPWORDS.remove('not')\n",
        "LEMMATIZER = WordNetLemmatizer ()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0RYG6KR84lr",
        "outputId": "5601a374-2061-47d8-a081-59d380dd1780"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Reading Documents"
      ],
      "metadata": {
        "id": "leREr8Hq9qW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load documents from a specified directory\n",
        "def load_documents(directory):\n",
        "    documents = {}\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(directory, filename), 'r') as file:\n",
        "                documents[filename] = file.read()\n",
        "    return documents\n",
        "\n",
        "documents = load_documents('path_to_documents')"
      ],
      "metadata": {
        "id": "qPTtkn5Y9sbX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Text Cleaning"
      ],
      "metadata": {
        "id": "FnsJmnP985bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean and preprocess text (lowercase, tokenization, stopwords removal, and lemmatization)\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [LEMMATIZER.lemmatize(token) for token in tokens if token not in STOPWORDS]\n",
        "    return tokens\n",
        "\n",
        "cleaned_documents = {filename: clean_text(content) for filename, content in documents.items()}"
      ],
      "metadata": {
        "id": "rALrHDfY8re8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Inverted Index Construction"
      ],
      "metadata": {
        "id": "7txSWLSL89T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create an inverted index\n",
        "def create_inverted_index(documents):\n",
        "    inverted_index = defaultdict(set)\n",
        "    for filename, tokens in documents.items():\n",
        "        for word in tokens:\n",
        "            inverted_index[word].add(filename)\n",
        "    return inverted_index\n",
        "\n",
        "inverted_index = create_inverted_index(cleaned_documents)"
      ],
      "metadata": {
        "id": "QIxdOwsM87s1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Boolean Query Processing: AND Operation"
      ],
      "metadata": {
        "id": "EKTg0DHC9Bkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize all_documents with the set of all document filenames\n",
        "all_documents = set(documents.keys())\n",
        "\n",
        "# Function for 'AND' query (finds common documents for all terms)\n",
        "def and_query(terms, inverted_index):\n",
        "    result = inverted_index.get(terms[0], set())\n",
        "    for term in terms[1:]:\n",
        "        result &= inverted_index.get(term, set())\n",
        "    return result"
      ],
      "metadata": {
        "id": "5ZdfBoyt9AuQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Boolean Query Processing: OR Operation"
      ],
      "metadata": {
        "id": "OcYtfoU59FYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for 'OR' query (finds documents that contain any of the terms)\n",
        "def or_query(terms, inverted_index):\n",
        "    result = inverted_index.get(terms[0], set())\n",
        "    for term in terms[1:]:\n",
        "        result |= inverted_index.get(term, set())\n",
        "    return result"
      ],
      "metadata": {
        "id": "aSX0UIFq9Eq4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Boolean Query Processing: NOT Operation"
      ],
      "metadata": {
        "id": "1VHuKN3C9JA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for 'NOT' query (finds documents that do not contain the specified term)\n",
        "def not_query(term, inverted_index, all_documents):\n",
        "    return all_documents - inverted_index.get(term, set())"
      ],
      "metadata": {
        "id": "a-nGho2s9IPG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Boolean Query Processing: Convert “doc ids” to Filenames"
      ],
      "metadata": {
        "id": "1JeVDJ-m9Mfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert document IDs (filenames) to a list\n",
        "def convert_doc_ids_to_filenames(doc_ids):\n",
        "    return list(doc_ids)"
      ],
      "metadata": {
        "id": "fN44tRFf9LqO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: Main Function"
      ],
      "metadata": {
        "id": "yDBCsdqL9QcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process the query and execute the appropriate Boolean operation\n",
        "def process_query(query, inverted_index, all_documents):\n",
        "    # Tokenize and preprocess the query\n",
        "    terms = [LEMMATIZER.lemmatize(term) for term in word_tokenize(query.lower()) if term not in STOPWORDS]\n",
        "    if 'and' in terms:\n",
        "        terms.remove('and')\n",
        "        result = and_query(terms, inverted_index)\n",
        "    elif 'or' in terms:\n",
        "        terms.remove('or')\n",
        "        result = or_query(terms, inverted_index)\n",
        "    elif 'not' in terms:\n",
        "        terms.remove('not')\n",
        "        result = not_query(terms[0], inverted_index, all_documents)\n",
        "    else:\n",
        "        result = inverted_index.get(terms[0], set())\n",
        "    return convert_doc_ids_to_filenames(result)"
      ],
      "metadata": {
        "id": "Eh4SLHPy9Pxh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10: Writing Query Results to Check File"
      ],
      "metadata": {
        "id": "b4pi1wVg9VA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query = \"not amy\"\n",
        "result = process_query(query, inverted_index, all_documents)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za_iwJhh9UTf",
        "outputId": "187cd1ae-387e-4a91-a96d-c1cbfa77f453"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Doc7.txt', 'Doc3.txt', 'Doc8.txt', 'Doc4.txt', 'Doc2.txt', 'Doc10.txt', 'Doc1.txt', 'Doc6.txt']\n"
          ]
        }
      ]
    }
  ]
}